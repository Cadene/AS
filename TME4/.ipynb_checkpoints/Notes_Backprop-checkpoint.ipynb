{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes sur la back propagation et chaine rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Étudiants Rémi Cadène et Mickael Chen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/torch/nn/blob/master/Module.lua\n",
    "\n",
    "Un module torch possède deux fonctions clés :\n",
    "\n",
    "```lua\n",
    "function Module:forward(input)\n",
    "   return self:updateOutput(input)\n",
    "end\n",
    "\n",
    "function Module:backward(input, gradOutput, scale)\n",
    "   scale = scale or 1 -- osef, toujours à 1\n",
    "   self:updateGradInput(input, gradOutput) -- maj de self.gradInput\n",
    "   self:accGradParameters(input, gradOutput, scale)\n",
    "   return self.gradInput\n",
    "end\n",
    "```\n",
    "\n",
    "Vous vous demandez quel est le rapport avec ça ?\n",
    "$$\\frac{\\partial\\bigtriangleup(f_\\theta(x), y)}{\\partial\\theta^m_{i,j}} = \\frac{\\partial\\bigtriangleup(f_\\theta(x), y)}{\\partial\\hat{y^m_{j}}} * \\frac{\\partial\\hat{y^m_{j}}}{\\partial\\theta^m_{i,j}}$$\n",
    "\n",
    "Et qu'est ce que sont $gradOuput$ et $gradInput$ ? Alors ce tutoriel est fait pour vous.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithmiquement, pour une full forward-backward pass d'un module linéaire, on a :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "require 'nn'\n",
    "\n",
    "dimInput = 6\n",
    "dimOutput = 3\n",
    "learningRate = 1e-2\n",
    "input = torch.Tensor{1,2,3,4,5,6}\n",
    "label = torch.Tensor{0,3,2}\n",
    "\n",
    "modele = nn.Linear(dimInput, dimOutput)\n",
    "criterion = nn.MSECriterion()\n",
    "\n",
    "modele:zeroGradParameters()\n",
    "output = modele:forward(input)\n",
    "loss = criterion:forward(output, label)\n",
    "dloss_do = criterion:backward(output, label)\n",
    "dloss_di = modele:backward(input, dloss_do)\n",
    "modele:updateParameters(learningRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- input est un vecteur de taille dimInput\n",
    "- output et label sont des vecteurs de tailles dimOutput\n",
    "- loss est un scalaire\n",
    "- df_do est un vecteur de taille dimOutput, c'est donc la dérivée du loss en fonction de l'output càd gradOuput\n",
    "- df_di est un vecteur de taille dimInput, c'est donc la dérivée du loss en fonction de l'input càd gradInput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De même, pour une full forward-backward pass d'un réseau de neurones à une couche cachée, on a :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dimHidden = 10\n",
    "module_1 = nn.Linear(dimInput, dimHidden)\n",
    "module_2 = nn.Tanh()\n",
    "module_3 = nn.Linear(dimHidden, dimOutput)\n",
    "criterion = nn.MSECriterion()\n",
    "module_1:zeroGradParameters()\n",
    "module_3:zeroGradParameters()\n",
    "output_m1  = module_1:forward(input)\n",
    "output_m2  = module_2:forward(output_m1)\n",
    "output     = module_3:forward(output_m2)\n",
    "loss       = criterion:forward(output, label)\n",
    "dloss_do   = criterion:backward(output, label)\n",
    "dloss_di   = module_3:backward(output_m2, dloss_do)\n",
    "dloss_dom1 = module_2:backward(output_m1, dloss_di)\n",
    "dloss_dom2 = module_1:backward(input, dloss_dom1)\n",
    "module_1:updateParameters(learningRate)\n",
    "module_3:updateParameters(learningRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque notamment que dloss_di est le gradInput du module_3 calculé par ce dernier et qu'il devient le gradOutput du module_2 (précédent). C'est ce processus qu'on appelle \"backprop chain-rule\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathématiquement, pour une full forward-backward pass d'un réseau de neurones à une couche cachée, on a :\n",
    "\n",
    "- soit $x^m_i$ la i-ème entrée du vecteur entrée $x^m$ du module $m$\n",
    "- soit $x$ le vecteur entrée du premier module $x^1$\n",
    "- soit $f_\\theta(x)$ l'output du modèle\n",
    "- soit $\\bigtriangleup(f_\\theta(x), y)$ l'erreur totale du modèle càd le loss\n",
    "- soit $\\theta^m_{i,j}$ le paramètre du module $m$ au rang $i,j$ de la matrice de poids $\\theta^m$ \n",
    "- soit $\\hat{y^m_j}$ la j-ème sortie du vecteur sortie $\\hat{y^m}$ du module $m$\n",
    "- soit $y$ le vecteur label ou sortie du dernier module $\\hat{y^3}$\n",
    "- soit $*$ le produit matriciel\n",
    "\n",
    "Notons que le terme module fait référence à une couche d'un réseau de neurone et modèle fait référence à toutes les couches. Càd $f_\\theta(x)$ fait référence à la sortie du modèle, tandis que $\\hat{y^m}$ fait référence à la sortie d'un module. Ainsi la sortie du dernier module est égale à la sortie du modèle. \n",
    "\n",
    "Maintenant, on veut calculer pour chaque poids $\\theta^m_{i,j}$ du modèle son $gradient$ càd $\\frac{\\partial\\bigtriangleup(f_\\theta(x), y)}{\\partial\\theta^m_{i,j}}$.\n",
    "\n",
    "On rappel, qu'algorithmiquement, ce calcul est effectué lors de l'appel à la fonction $self:accGradParameters(input, gradOutput)$.\n",
    "\n",
    "##### On sait que $input$ est l'entrée $x^m$, mais qu'est ce que $gradOuput$ à part le $gradInput$ du module suivant ?\n",
    "\n",
    "Pour comprendre, on décompose : $$\\frac{\\partial\\bigtriangleup(f_\\theta(x), y)}{\\partial\\theta^m_{i,j}} = \\frac{\\partial\\bigtriangleup(f_\\theta(x), y)}{\\partial\\hat{y^m_{j}}} * \\frac{\\partial\\hat{y^m_{j}}}{\\partial\\theta^m_{i,j}}$$\n",
    "\n",
    "On voit apparaître deux termes :\n",
    "-  $\\frac{\\partial\\hat{y^m_{i,j}}}{\\partial\\theta^m_{i,j}}$ qui est la dérivée de la sortie du module donc de la fonction $f^m(x^m)$ en fonction de chaque paramètre du module $\\theta^m_{i,j}$. Ainsi, $f^m$ peut être la fonction de coût (criterion), ou la multiplication matricielle d'un module linéaire, ou encore la fonction sigmoïde du module $nn.Tanh$ (cette dernière n'ayant pas de paramètres, sa dérivée sera nulle).\n",
    "- $\\frac{\\partial\\bigtriangleup(f_\\theta(x), y)}{\\partial\\hat{y^m_{j}}}$ qui est la dérivée de l'erreur du modèle en fonction de la sortie du module $m$, mais encore de l'entrée du module $m+1$. Ainsi pour obtenir ce terme, nous devons dériver l'erreur du modèle en fonction de l'entrée $x^{m+1}$. Cette dérivée est calculée lors de l'appel à la fonction $self:updateGradInput(input, gradOutput)$ du module suivant.\n",
    "\n",
    "##### Mais qu'est ce que cette dérivée du loss en fonction de l'entrée du module $m+1$ ?\n",
    "\n",
    "Pour comprendre, on décompose : $$\\frac{\\partial\\bigtriangleup(f_\\theta(x), y)}{\\partial{x^{m+1}_i}} = \\frac{\\partial\\bigtriangleup(f_\\theta(x), y)}{\\partial\\hat{y^{m+1}_{j}}} * \\frac{\\partial\\hat{y^{m+1}_{j}}}{\\partial{x^{m+1}_i}}$$\n",
    "\n",
    "On retrouve le terme $\\frac{\\partial\\bigtriangleup(f_\\theta(x), y)}{\\partial\\hat{y^{m+1}_{j}}}$ qui est donc le fameux $gradOuput$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "Pour résumer, un module $m$ doit faire deux trucs :\n",
    "1. Calculer son $gradInput$ avec la méthode $updateGradInput(input, gradOutput)$ ($gradOuput$ étant le $gradInput$ du module $m+1$). Ce qui revient à calculer $\\frac{\\partial\\hat{y^{m}_{j}}}{\\partial{x^{m}_i}}$, puis à le multiplier par $gradOutput$.\n",
    "2. Calculer son $gradParameters$ avec la méthode $accGradParameters(input, gradOutput)$. Ce qui revient à calculer $\\frac{\\partial\\hat{y^m_{j}}}{\\partial\\theta^m_{i,j}}$, puis à le mutliplier par $gradOuput$.\n",
    "\n",
    "Beware of the reshape.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### J'espère que cela aura été usefull :) bon courage pour l'implémentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
